{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7118738-0d33-4c68-95f0-3be1900b9cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "class2 = pd.read_csv(\"C:\\\\Users\\\\516-29\\\\chap10\\\\data\\\\class2.csv\")\n",
    "\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "onehot_encoder = preprocessing.OneHotEncoder()\n",
    "train_x = label_encoder.fit_transform(class2['class2'])\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b38a61b-961d-43e9-9814-826b7230a7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 13,\n",
       " 'is': 7,\n",
       " 'last': 8,\n",
       " 'chance': 2,\n",
       " 'and': 0,\n",
       " 'if': 6,\n",
       " 'you': 15,\n",
       " 'do': 3,\n",
       " 'not': 10,\n",
       " 'have': 5,\n",
       " 'will': 14,\n",
       " 'never': 9,\n",
       " 'get': 4,\n",
       " 'any': 1,\n",
       " 'one': 11,\n",
       " 'please': 12}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is last chance.',\n",
    "    'and if you do not have this chance.',\n",
    "    'you will never get any chance.',\n",
    "    'will you do get this one?',\n",
    "    'please, get this chance',\n",
    "]\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "826b7263-1f61-4e29-a760-f4fcb4df79ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(['you will never get any chance.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ce59b7-a28d-4445-9e0e-db08747171b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last': 6,\n",
       " 'chance': 1,\n",
       " 'if': 5,\n",
       " 'you': 11,\n",
       " 'do': 2,\n",
       " 'not': 8,\n",
       " 'have': 4,\n",
       " 'will': 10,\n",
       " 'never': 7,\n",
       " 'get': 3,\n",
       " 'any': 0,\n",
       " 'one': 9}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=[\"and\", \"is\", \"please\", \"this\"]).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4511d311-1136-486e-a445-e08913a88366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유사도를 위한 3 x 3 행렬을 만들었습니다.\n",
      "[[1.       0.224325 0.      ]\n",
      " [0.224325 1.       0.      ]\n",
      " [0.       0.       1.      ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "doc = ['I like machine learning', 'I love deep learning', 'I run everyday']\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=1)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(doc)\n",
    "doc_distance = (tfidf_matrix * tfidf_matrix.T)\n",
    "\n",
    "print('유사도를 위한', str(doc_distance.get_shape()[0]), 'x', str(doc_distance.get_shape()[1]), '행렬을 만들었습니다.')\n",
    "print(doc_distance.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4228fb6a-f8ed-4797-ad65-9e50db8dcb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['once',\n",
       "  'upon',\n",
       "  'a',\n",
       "  'time',\n",
       "  'in',\n",
       "  'london',\n",
       "  ',',\n",
       "  'the',\n",
       "  'darlings',\n",
       "  'went',\n",
       "  'out',\n",
       "  'to',\n",
       "  'a',\n",
       "  'dinner',\n",
       "  'party',\n",
       "  'leaving',\n",
       "  'their',\n",
       "  'three',\n",
       "  'children',\n",
       "  'wendy',\n",
       "  ',',\n",
       "  'jhon',\n",
       "  ',',\n",
       "  'and',\n",
       "  'michael',\n",
       "  'at',\n",
       "  'home',\n",
       "  '.'],\n",
       " ['after',\n",
       "  'wendy',\n",
       "  'had',\n",
       "  'tucked',\n",
       "  'her',\n",
       "  'younger',\n",
       "  'brothers',\n",
       "  'jhon',\n",
       "  'and',\n",
       "  'michael',\n",
       "  'to',\n",
       "  'bed',\n",
       "  ',',\n",
       "  'she',\n",
       "  'went',\n",
       "  'to',\n",
       "  'read',\n",
       "  'a',\n",
       "  'book',\n",
       "  '.'],\n",
       " ['she', 'heard', 'a', 'boy', 'sobbing', 'outside', 'her', 'window', '.'],\n",
       " ['he', 'was', 'flying', '.'],\n",
       " ['there', 'was', 'little', 'fairy', 'fluttering', 'around', 'him', '.'],\n",
       " ['wendy', 'opened', 'the', 'window', 'to', 'talk', 'to', 'him', '.'],\n",
       " ['“', 'hello', '!'],\n",
       " ['who', 'are', 'you', '?'],\n",
       " ['why', 'are', 'you', 'crying', '”', ',', 'wendy', 'asked', 'him', '.'],\n",
       " ['“', 'my', 'name', 'is', 'peter', 'pan', '.'],\n",
       " ['my',\n",
       "  'shadow',\n",
       "  'wouldn',\n",
       "  '’',\n",
       "  't',\n",
       "  'stock',\n",
       "  'to',\n",
       "  'me.',\n",
       "  '”',\n",
       "  ',',\n",
       "  'he',\n",
       "  'replied',\n",
       "  '.'],\n",
       " ['she', 'asked', 'him', 'to', 'come', 'in', '.'],\n",
       " ['peter', 'agreed', 'and', 'came', 'inside', 'the', 'room', '.'],\n",
       " ['wendy',\n",
       "  'took',\n",
       "  'his',\n",
       "  'shadow',\n",
       "  'and',\n",
       "  'sewed',\n",
       "  'it',\n",
       "  'to',\n",
       "  'his',\n",
       "  'shoe',\n",
       "  'tips',\n",
       "  '.'],\n",
       " ['now',\n",
       "  'his',\n",
       "  'shadow',\n",
       "  'followed',\n",
       "  'him',\n",
       "  'wherever',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'went',\n",
       "  '!'],\n",
       " ['he',\n",
       "  'was',\n",
       "  'delighted',\n",
       "  'and',\n",
       "  'asked',\n",
       "  'wendy',\n",
       "  '“',\n",
       "  'why',\n",
       "  'don',\n",
       "  '’',\n",
       "  't',\n",
       "  'you',\n",
       "  'come',\n",
       "  'with',\n",
       "  'me',\n",
       "  'to',\n",
       "  'my',\n",
       "  'home',\n",
       "  '.'],\n",
       " ['the', 'neverland', '.'],\n",
       " ['i',\n",
       "  'lived',\n",
       "  'there',\n",
       "  'with',\n",
       "  'my',\n",
       "  'fairy',\n",
       "  'tinker',\n",
       "  'bell.',\n",
       "  '”',\n",
       "  'wendy',\n",
       "  '?'],\n",
       " ['“', 'oh', '!'],\n",
       " ['what', 'a', 'wonderful', 'idea', '!'],\n",
       " ['let', 'me', 'wake', 'up', 'john', 'and', 'micheal', 'too', '.'],\n",
       " ['could', 'you', 'teach', 'us', 'how', 'to', 'fly', '?', '”', '.'],\n",
       " ['“', 'yes', '!'],\n",
       " ['of', 'course', '!'],\n",
       " ['get',\n",
       "  'them',\n",
       "  'we',\n",
       "  'will',\n",
       "  'all',\n",
       "  'fly',\n",
       "  'together.',\n",
       "  '”',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'replied',\n",
       "  'and',\n",
       "  'so',\n",
       "  'it',\n",
       "  'was',\n",
       "  '.'],\n",
       " ['five',\n",
       "  'little',\n",
       "  'figures',\n",
       "  'flew',\n",
       "  'out',\n",
       "  'of',\n",
       "  'the',\n",
       "  'window',\n",
       "  'of',\n",
       "  'the',\n",
       "  'darlings',\n",
       "  'and',\n",
       "  'headed',\n",
       "  'towards',\n",
       "  'neverland',\n",
       "  '.'],\n",
       " ['as',\n",
       "  'they',\n",
       "  'flew',\n",
       "  'over',\n",
       "  'the',\n",
       "  'island',\n",
       "  ',',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'told',\n",
       "  'the',\n",
       "  'children',\n",
       "  'more',\n",
       "  'about',\n",
       "  'his',\n",
       "  'homeland',\n",
       "  '.'],\n",
       " ['“',\n",
       "  'all',\n",
       "  'the',\n",
       "  'children',\n",
       "  'who',\n",
       "  'get',\n",
       "  'lost',\n",
       "  'come',\n",
       "  'and',\n",
       "  'stay',\n",
       "  'with',\n",
       "  'tinker',\n",
       "  'bell',\n",
       "  'and',\n",
       "  'me',\n",
       "  ',',\n",
       "  '”',\n",
       "  'peter',\n",
       "  'told',\n",
       "  'them',\n",
       "  '.'],\n",
       " ['the', 'indians', 'also', 'live', 'in', 'neverland', '.'],\n",
       " ['the',\n",
       "  'mermaids',\n",
       "  'live',\n",
       "  'in',\n",
       "  'the',\n",
       "  'lagoon',\n",
       "  'around',\n",
       "  'the',\n",
       "  'island',\n",
       "  '.'],\n",
       " ['and',\n",
       "  'a',\n",
       "  'very',\n",
       "  'mean',\n",
       "  'pirate',\n",
       "  'called',\n",
       "  'captain',\n",
       "  'hook',\n",
       "  'keeps',\n",
       "  'troubling',\n",
       "  'everyone',\n",
       "  '.'],\n",
       " ['“', 'crocodile', 'bit', 'his', 'one', 'arm', '.'],\n",
       " ['so',\n",
       "  'the',\n",
       "  'captain',\n",
       "  'had',\n",
       "  'to',\n",
       "  'put',\n",
       "  'a',\n",
       "  'hook',\n",
       "  'in',\n",
       "  'its',\n",
       "  'place',\n",
       "  '.'],\n",
       " ['since', 'then', 'he', 'is', 'afraid', 'of', 'crocodiles', '.'],\n",
       " ['and', 'rightly', 'so', '!'],\n",
       " ['if',\n",
       "  'the',\n",
       "  'crocodile',\n",
       "  'ever',\n",
       "  'found',\n",
       "  'captain',\n",
       "  'hook',\n",
       "  'it',\n",
       "  'will',\n",
       "  'eat',\n",
       "  'up',\n",
       "  'the',\n",
       "  'rest',\n",
       "  'of',\n",
       "  'it',\n",
       "  'couldn',\n",
       "  '’',\n",
       "  't',\n",
       "  'eat',\n",
       "  'last',\n",
       "  'time.',\n",
       "  '”',\n",
       "  'peter',\n",
       "  'told',\n",
       "  'them',\n",
       "  '.'],\n",
       " ['soon', 'they', 'landed', 'on', 'the', 'island', '.'],\n",
       " ['and',\n",
       "  'to',\n",
       "  'the',\n",
       "  'surprise',\n",
       "  'of',\n",
       "  'wendy',\n",
       "  ',',\n",
       "  'jhon',\n",
       "  'and',\n",
       "  'michael',\n",
       "  ',',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'let',\n",
       "  'them',\n",
       "  'in',\n",
       "  'through',\n",
       "  'a',\n",
       "  'small',\n",
       "  'opening',\n",
       "  'in',\n",
       "  'a',\n",
       "  'tree',\n",
       "  '.'],\n",
       " ['inside',\n",
       "  'the',\n",
       "  'tree',\n",
       "  'was',\n",
       "  'a',\n",
       "  'large',\n",
       "  'room',\n",
       "  'with',\n",
       "  'children',\n",
       "  'inside',\n",
       "  'it',\n",
       "  '.'],\n",
       " ['somewhere',\n",
       "  'huddled',\n",
       "  'by',\n",
       "  'the',\n",
       "  'fire',\n",
       "  'in',\n",
       "  'the',\n",
       "  'corner',\n",
       "  'and',\n",
       "  'somewhere',\n",
       "  'playing',\n",
       "  'amongst',\n",
       "  'themselves',\n",
       "  '.'],\n",
       " ['their',\n",
       "  'faces',\n",
       "  'lit',\n",
       "  'up',\n",
       "  'when',\n",
       "  'they',\n",
       "  'saw',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  ',',\n",
       "  'tinker',\n",
       "  'bell',\n",
       "  ',',\n",
       "  'and',\n",
       "  'their',\n",
       "  'guests',\n",
       "  '.'],\n",
       " ['“', 'hello', 'everyone', '.'],\n",
       " ['this', 'is', 'wendy', ',', 'jhon', ',', 'and', 'michael', '.'],\n",
       " ['they',\n",
       "  'will',\n",
       "  'be',\n",
       "  'staying',\n",
       "  'with',\n",
       "  'us',\n",
       "  'from',\n",
       "  'now',\n",
       "  'on.',\n",
       "  '”',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'introduced',\n",
       "  'them',\n",
       "  'to',\n",
       "  'all',\n",
       "  'children',\n",
       "  '.'],\n",
       " ['children', 'welcomed', 'wendy', ',', 'jhon', ',', 'and', 'michael', '.'],\n",
       " ['a', 'few', 'days', 'passed', '.'],\n",
       " ['and', 'they', 'settled', 'into', 'a', 'routine', '.'],\n",
       " ['wendy',\n",
       "  'would',\n",
       "  'take',\n",
       "  'care',\n",
       "  'of',\n",
       "  'all',\n",
       "  'the',\n",
       "  'children',\n",
       "  'in',\n",
       "  'the',\n",
       "  'day',\n",
       "  'and',\n",
       "  'would',\n",
       "  'go',\n",
       "  'out',\n",
       "  'with',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'and',\n",
       "  'her',\n",
       "  'brothers',\n",
       "  'in',\n",
       "  'the',\n",
       "  'evening',\n",
       "  'to',\n",
       "  'learn',\n",
       "  'about',\n",
       "  'the',\n",
       "  'island',\n",
       "  '.'],\n",
       " ['she',\n",
       "  'would',\n",
       "  'cook',\n",
       "  'for',\n",
       "  'them',\n",
       "  'and',\n",
       "  'stitch',\n",
       "  'new',\n",
       "  'clothes',\n",
       "  'for',\n",
       "  'them',\n",
       "  '.'],\n",
       " ['he',\n",
       "  'even',\n",
       "  'made',\n",
       "  'a',\n",
       "  'lovely',\n",
       "  'new',\n",
       "  'dress',\n",
       "  'for',\n",
       "  'tinker',\n",
       "  'bell',\n",
       "  '.'],\n",
       " ['one',\n",
       "  'evening',\n",
       "  ',',\n",
       "  'as',\n",
       "  'they',\n",
       "  'were',\n",
       "  'out',\n",
       "  'exploring',\n",
       "  'the',\n",
       "  'island',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'warned',\n",
       "  'everyone',\n",
       "  'and',\n",
       "  'said',\n",
       "  ',',\n",
       "  '“',\n",
       "  'hide',\n",
       "  '!'],\n",
       " ['hide', '!'],\n",
       " ['pirates', '!'],\n",
       " ['and',\n",
       "  'they',\n",
       "  'have',\n",
       "  'kidnapped',\n",
       "  'the',\n",
       "  'indian',\n",
       "  'princess',\n",
       "  'tiger',\n",
       "  'lily',\n",
       "  '.'],\n",
       " ['they',\n",
       "  'have',\n",
       "  'kept',\n",
       "  'her',\n",
       "  'there',\n",
       "  ',',\n",
       "  'tied',\n",
       "  'up',\n",
       "  'by',\n",
       "  'the',\n",
       "  'rocks',\n",
       "  ',',\n",
       "  'near',\n",
       "  'the',\n",
       "  'water.',\n",
       "  '”',\n",
       "  'peter',\n",
       "  'was',\n",
       "  'afraid',\n",
       "  'and',\n",
       "  'the',\n",
       "  'princess',\n",
       "  'would',\n",
       "  'drown',\n",
       "  ',',\n",
       "  'is',\n",
       "  'she',\n",
       "  'fell',\n",
       "  'into',\n",
       "  'the',\n",
       "  'water',\n",
       "  '.'],\n",
       " ['so',\n",
       "  ',',\n",
       "  'in',\n",
       "  'a',\n",
       "  'voice',\n",
       "  'that',\n",
       "  'sounded',\n",
       "  'like',\n",
       "  'captain',\n",
       "  'hook',\n",
       "  ',',\n",
       "  'he',\n",
       "  'shouted',\n",
       "  'instructions',\n",
       "  'to',\n",
       "  'the',\n",
       "  'pirates',\n",
       "  'who',\n",
       "  'guarded',\n",
       "  'her',\n",
       "  ',',\n",
       "  '“',\n",
       "  'you',\n",
       "  'fools',\n",
       "  '!'],\n",
       " ['let', 'her', 'go', 'at', 'once', '!'],\n",
       " ['do',\n",
       "  'it',\n",
       "  'before',\n",
       "  'i',\n",
       "  'come',\n",
       "  'there',\n",
       "  'or',\n",
       "  'else',\n",
       "  'i',\n",
       "  'will',\n",
       "  'throw',\n",
       "  'each',\n",
       "  'one',\n",
       "  'of',\n",
       "  'you',\n",
       "  'into',\n",
       "  'the',\n",
       "  'water.',\n",
       "  '”',\n",
       "  'the',\n",
       "  'pirates',\n",
       "  'got',\n",
       "  'scared',\n",
       "  'and',\n",
       "  'immediately',\n",
       "  'released',\n",
       "  'the',\n",
       "  'princes',\n",
       "  '.'],\n",
       " ['she',\n",
       "  'quickly',\n",
       "  'dived',\n",
       "  'into',\n",
       "  'the',\n",
       "  'water',\n",
       "  'and',\n",
       "  'swam',\n",
       "  'to',\n",
       "  'the',\n",
       "  'safety',\n",
       "  'of',\n",
       "  'her',\n",
       "  'home',\n",
       "  '.'],\n",
       " ['soon',\n",
       "  'everyone',\n",
       "  'found',\n",
       "  'out',\n",
       "  'how',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'had',\n",
       "  'rescued',\n",
       "  'the',\n",
       "  'princess',\n",
       "  '.'],\n",
       " ['when',\n",
       "  'captain',\n",
       "  'hook',\n",
       "  'found',\n",
       "  'out',\n",
       "  'how',\n",
       "  'peter',\n",
       "  'had',\n",
       "  'tricked',\n",
       "  'his',\n",
       "  'men',\n",
       "  'he',\n",
       "  'was',\n",
       "  'furious',\n",
       "  '.'],\n",
       " ['and', 'swore', 'to', 'have', 'his', 'revenge', '.'],\n",
       " ['that',\n",
       "  'night',\n",
       "  'wendy',\n",
       "  'told',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  ',',\n",
       "  'that',\n",
       "  'she',\n",
       "  'and',\n",
       "  'her',\n",
       "  'brother',\n",
       "  'wanted',\n",
       "  'to',\n",
       "  'go',\n",
       "  'back',\n",
       "  'home',\n",
       "  'since',\n",
       "  'they',\n",
       "  'missed',\n",
       "  'their',\n",
       "  'parents',\n",
       "  '.'],\n",
       " ['she',\n",
       "  'said',\n",
       "  'if',\n",
       "  'the',\n",
       "  'lost',\n",
       "  'children',\n",
       "  'could',\n",
       "  'also',\n",
       "  'return',\n",
       "  'to',\n",
       "  'her',\n",
       "  'world',\n",
       "  'they',\n",
       "  'could',\n",
       "  'find',\n",
       "  'a',\n",
       "  'nice',\n",
       "  'home',\n",
       "  'for',\n",
       "  'them',\n",
       "  '.'],\n",
       " ['peter', 'pan', 'didn', '’', 't', 'want', 'to', 'leave', 'neverland', '.'],\n",
       " ['but',\n",
       "  'the',\n",
       "  'sake',\n",
       "  'of',\n",
       "  'the',\n",
       "  'lost',\n",
       "  'children',\n",
       "  'he',\n",
       "  'agreed',\n",
       "  ',',\n",
       "  'although',\n",
       "  'a',\n",
       "  'bit',\n",
       "  'sadly',\n",
       "  '.'],\n",
       " ['he', 'would', 'miss', 'his', 'friends', 'dearly', '.'],\n",
       " ['the',\n",
       "  'next',\n",
       "  'morning',\n",
       "  'all',\n",
       "  'the',\n",
       "  'lost',\n",
       "  'children',\n",
       "  'left',\n",
       "  'with',\n",
       "  'wendy',\n",
       "  ',',\n",
       "  'jhon',\n",
       "  ',',\n",
       "  'and',\n",
       "  'michael',\n",
       "  '.'],\n",
       " ['but',\n",
       "  'on',\n",
       "  'the',\n",
       "  'way',\n",
       "  ',',\n",
       "  'captain',\n",
       "  'hook',\n",
       "  'and',\n",
       "  'his',\n",
       "  'men',\n",
       "  'kidnapped',\n",
       "  'all',\n",
       "  'of',\n",
       "  'them',\n",
       "  '.'],\n",
       " ['he',\n",
       "  'tied',\n",
       "  'them',\n",
       "  'and',\n",
       "  'kept',\n",
       "  'them',\n",
       "  'on',\n",
       "  'once',\n",
       "  'of',\n",
       "  'his',\n",
       "  'ships',\n",
       "  '.'],\n",
       " ['as',\n",
       "  'soon',\n",
       "  'as',\n",
       "  'peter',\n",
       "  'found',\n",
       "  'out',\n",
       "  'about',\n",
       "  'it',\n",
       "  'he',\n",
       "  'rushed',\n",
       "  'to',\n",
       "  'the',\n",
       "  'ship',\n",
       "  '.'],\n",
       " ['he',\n",
       "  'swung',\n",
       "  'himself',\n",
       "  'from',\n",
       "  'a',\n",
       "  'tress',\n",
       "  'branch',\n",
       "  'and',\n",
       "  'on',\n",
       "  'to',\n",
       "  'the',\n",
       "  'deck',\n",
       "  'of',\n",
       "  'the',\n",
       "  'ship',\n",
       "  'where',\n",
       "  'all',\n",
       "  'the',\n",
       "  'children',\n",
       "  'were',\n",
       "  'tied',\n",
       "  'up',\n",
       "  '.'],\n",
       " ['he',\n",
       "  'swung',\n",
       "  'his',\n",
       "  'sword',\n",
       "  'bravely',\n",
       "  'and',\n",
       "  'threw',\n",
       "  'over',\n",
       "  'the',\n",
       "  'pirates',\n",
       "  'who',\n",
       "  'tried',\n",
       "  'to',\n",
       "  'stop',\n",
       "  'him',\n",
       "  '.'],\n",
       " ['quickly',\n",
       "  'he',\n",
       "  'released',\n",
       "  'everyone',\n",
       "  'from',\n",
       "  'their',\n",
       "  'captor',\n",
       "  '’',\n",
       "  's',\n",
       "  'ties',\n",
       "  '.'],\n",
       " ['wendy',\n",
       "  ',',\n",
       "  'jhon',\n",
       "  ',',\n",
       "  'michael',\n",
       "  'and',\n",
       "  'tinker',\n",
       "  'bell',\n",
       "  'helped',\n",
       "  'all',\n",
       "  'the',\n",
       "  'children',\n",
       "  'into',\n",
       "  'the',\n",
       "  'water',\n",
       "  ',',\n",
       "  'where',\n",
       "  'their',\n",
       "  'friends',\n",
       "  'from',\n",
       "  'the',\n",
       "  'indian',\n",
       "  'camp',\n",
       "  'were',\n",
       "  'ready',\n",
       "  'with',\n",
       "  'smaller',\n",
       "  'boats',\n",
       "  'to',\n",
       "  'take',\n",
       "  'them',\n",
       "  'to',\n",
       "  'safety',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'now',\n",
       "  'went',\n",
       "  'looking',\n",
       "  'for',\n",
       "  'captain',\n",
       "  'hook',\n",
       "  '.'],\n",
       " ['“',\n",
       "  'let',\n",
       "  'us',\n",
       "  'finished',\n",
       "  'this',\n",
       "  'forever',\n",
       "  'mr.',\n",
       "  'hook',\n",
       "  '”',\n",
       "  ',',\n",
       "  'peter',\n",
       "  'challenged',\n",
       "  'captain',\n",
       "  'hook',\n",
       "  '.'],\n",
       " ['“', 'yes', '!'],\n",
       " ['peter',\n",
       "  'pan',\n",
       "  ',',\n",
       "  'you',\n",
       "  'have',\n",
       "  'caused',\n",
       "  'me',\n",
       "  'enough',\n",
       "  'trouble',\n",
       "  '.'],\n",
       " ['it',\n",
       "  'is',\n",
       "  'time',\n",
       "  'that',\n",
       "  'we',\n",
       "  'finished',\n",
       "  'this.',\n",
       "  '”',\n",
       "  'hook',\n",
       "  'replied',\n",
       "  '.'],\n",
       " ['with',\n",
       "  'his',\n",
       "  'sword',\n",
       "  'drawn',\n",
       "  ',',\n",
       "  'he',\n",
       "  'raced',\n",
       "  'towards',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  '.'],\n",
       " ['quick',\n",
       "  'on',\n",
       "  'his',\n",
       "  'feet',\n",
       "  ',',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'stepped',\n",
       "  'aside',\n",
       "  'and',\n",
       "  'pushed',\n",
       "  'hook',\n",
       "  'inside',\n",
       "  'the',\n",
       "  'sea',\n",
       "  'where',\n",
       "  'the',\n",
       "  'crocodile',\n",
       "  'was',\n",
       "  'waiting',\n",
       "  'to',\n",
       "  'eat',\n",
       "  'the',\n",
       "  'rest',\n",
       "  'of',\n",
       "  'hook',\n",
       "  '.'],\n",
       " ['everyone',\n",
       "  'rejoiced',\n",
       "  'as',\n",
       "  'captain',\n",
       "  'hook',\n",
       "  'was',\n",
       "  'out',\n",
       "  'of',\n",
       "  'their',\n",
       "  'lives',\n",
       "  'forever',\n",
       "  '.'],\n",
       " ['everybody', 'headed', 'back', 'to', 'london', '.'],\n",
       " ['mr.', 'and', 'mrs', '.'],\n",
       " ['darling',\n",
       "  'was',\n",
       "  'so',\n",
       "  'happy',\n",
       "  'to',\n",
       "  'see',\n",
       "  'their',\n",
       "  'children',\n",
       "  'and',\n",
       "  'they',\n",
       "  'agreed',\n",
       "  'to',\n",
       "  'adopt',\n",
       "  'the',\n",
       "  'lost',\n",
       "  'children',\n",
       "  '.'],\n",
       " ['they',\n",
       "  'even',\n",
       "  'asked',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'to',\n",
       "  'come',\n",
       "  'and',\n",
       "  'live',\n",
       "  'with',\n",
       "  'them',\n",
       "  '.'],\n",
       " ['but',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'said',\n",
       "  ',',\n",
       "  'he',\n",
       "  'never',\n",
       "  'wanted',\n",
       "  'to',\n",
       "  'grow',\n",
       "  'up',\n",
       "  ',',\n",
       "  'so',\n",
       "  'he',\n",
       "  'and',\n",
       "  'tinker',\n",
       "  'bell',\n",
       "  'will',\n",
       "  'go',\n",
       "  'back',\n",
       "  'to',\n",
       "  'neverland',\n",
       "  '.'],\n",
       " ['peter',\n",
       "  'pan',\n",
       "  'promised',\n",
       "  'everyone',\n",
       "  'that',\n",
       "  'he',\n",
       "  'will',\n",
       "  'visit',\n",
       "  'again',\n",
       "  'sometime',\n",
       "  '!'],\n",
       " ['and',\n",
       "  'he',\n",
       "  'flew',\n",
       "  'out',\n",
       "  'of',\n",
       "  'the',\n",
       "  'window',\n",
       "  'with',\n",
       "  'tinker',\n",
       "  'bell',\n",
       "  'by',\n",
       "  'his',\n",
       "  'side',\n",
       "  '.']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sample = open(\"C:\\\\Users\\\\516-29\\\\chap10\\\\data\\\\peter.txt\", \"r\", encoding='UTF8')\n",
    "s = sample.read()\n",
    "\n",
    "f = s.replace(\"\\n\", \" \")\n",
    "data = []\n",
    "\n",
    "for i in sent_tokenize(f):\n",
    "    temp = []\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    "    data.append(temp)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0178a2c4-e641-4d9a-a1c6-5b2ab8eb91ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'peter' 'wendy' - CBOW :  -0.08294406\n"
     ]
    }
   ],
   "source": [
    "model1 = gensim.models.Word2Vec(data, min_count=1, vector_size=100, window=5, sg=0)\n",
    "\n",
    "print(\"Cosine similarity between 'peter' \" + \"'wendy' - CBOW : \",\n",
    "      model1.wv.similarity('peter', 'wendy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61bffb3e-86d6-443a-ac1a-4e5547d58893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'peter' 'hook' - CBOW :  -0.058279652\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'peter' \" + \"'hook' - CBOW : \", model1.wv.similarity('peter', 'hook'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9753bc3-5eca-4d82-b2e7-ec98a71f4db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'peter' 'wendy' - Skip Gram :  0.27075687\n"
     ]
    }
   ],
   "source": [
    "model2 = gensim.models.Word2Vec(data, min_count=1, vector_size=100, window=5, sg=1)\n",
    "\n",
    "print(\"Cosine similarity between 'peter' \" + \"'wendy' - Skip Gram : \", model2.wv.similarity('peter', 'wendy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a44697b-94dd-4c71-a8f3-469d9905717f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'peter' 'hook' - Skip Gram :  0.455733\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'peter' \" + \"'hook' - Skip Gram : \", model2.wv.similarity('peter', 'hook'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d590ccb-ec4e-45b3-b0c4-6c0b3ddce374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import FastText\n",
    "\n",
    "model = FastText('..\\chap10\\data\\peter.txt', vector_size=4, window=3, min_count=1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4acbb67-59c1-44bf-a760-23ab26cfbbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4592452\n"
     ]
    }
   ],
   "source": [
    "sim_score = model.wv.similarity('peter', 'wendy')\n",
    "print(sim_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32829035-c034-4154-9691-30896ccb8c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.043825716\n"
     ]
    }
   ],
   "source": [
    "sim_score = model.wv.similarity('peter', 'hook')\n",
    "print(sim_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dabfbba4-9d9c-4708-ac57-3df323a5dab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from gensim.models import KeyedVectors\n",
    "model_kr = KeyedVectors.load_word2vec_format(\n",
    "    'C:\\\\Users\\\\516-29\\\\chap10\\\\data\\\\wiki.ko.vec',\n",
    "    limit=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f703131e-69d1-4004-a3c7-065f082f5416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: 노력과, Similarity: 0.71\n",
      "Word: 노력의, Similarity: 0.69\n",
      "Word: 노력은, Similarity: 0.66\n",
      "Word: 노력이, Similarity: 0.64\n",
      "Word: 노력으로, Similarity: 0.64\n",
      "Word: 노력을, Similarity: 0.64\n",
      "Word: 노력하는, Similarity: 0.63\n",
      "Word: 노력에, Similarity: 0.62\n",
      "Word: 역량, Similarity: 0.60\n",
      "Word: 노력한, Similarity: 0.60\n"
     ]
    }
   ],
   "source": [
    "find_similar_to = '노력'\n",
    "\n",
    "for similar_word in model_kr.similar_by_word(find_similar_to):\n",
    "    print(\"Word: {0}, Similarity: {1:.2f}\".format(\n",
    "        similar_word[0], similar_word[1]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ed0717a-0419-4c1a-8934-f9293cf9ed44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = datapath('C:\\\\Users\\\\516-29\\\\chap10\\\\data\\\\glove.6B.100d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6362a33b-0c35-4299-9fbf-a44e8c8c3584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('legislation', 0.8072139620780945),\n",
       " ('proposal', 0.7306863069534302),\n",
       " ('senate', 0.7142541408538818),\n",
       " ('bills', 0.704440176486969),\n",
       " ('measure', 0.6958035230636597),\n",
       " ('passed', 0.6906244158744812),\n",
       " ('amendment', 0.6846879720687866),\n",
       " ('provision', 0.6845567226409912),\n",
       " ('plan', 0.6816462874412537),\n",
       " ('clinton', 0.6663140654563904)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
    "model.most_similar('bill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e998815-c251-46c5-92fe-b04a1131100f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('peach', 0.688809871673584),\n",
       " ('mango', 0.683819055557251),\n",
       " ('plum', 0.6684104204177856),\n",
       " ('berry', 0.659035861492157),\n",
       " ('grove', 0.6581551432609558),\n",
       " ('blossom', 0.6503506302833557),\n",
       " ('raspberry', 0.6477391719818115),\n",
       " ('strawberry', 0.6442098021507263),\n",
       " ('pine', 0.6390928626060486),\n",
       " ('almond', 0.6379212737083435)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('cherry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c277552b-92d4-4e60-abd9-a523750c02e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kazushige', 0.48343509435653687),\n",
       " ('askerov', 0.4778185784816742),\n",
       " ('lakpa', 0.46915262937545776),\n",
       " ('ex-gay', 0.45713329315185547),\n",
       " ('tadayoshi', 0.4522106647491455),\n",
       " ('turani', 0.4481006860733032),\n",
       " ('saglam', 0.4469599425792694),\n",
       " ('aijun', 0.4435270130634308),\n",
       " ('adjustors', 0.44235295057296753),\n",
       " ('nyum', 0.4423118233680725)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(negative=['cherry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57217788-372c-4854-92fb-1575ce630618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen: 0.7699\n"
     ]
    }
   ],
   "source": [
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7abf0ef1-7d02-4f74-a2d3-10b5d35ab945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'champagne'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analogy(x1, x2, y1):\n",
    "    result = model.most_similar(positive=[y1, x2], negative=[x1])\n",
    "    return result[0][0]\n",
    "analogy('australia', 'beer', 'france')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba98f826-e97c-4681-b5d8-34c900f57a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'longest'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('tall', 'tallest', 'long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d250212-1874-4a87-8e43-f44b95ebd264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cereal\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3de1dd73-0188-46a5-9c2f-345eb9ac9631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dfcbfd4-2f14-4530-8167-0a214353da7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 20\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e4c7614-3ab0-4756-9d51-56ebb953dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeString(df, lang):\n",
    "    sentence = df[lang].str.lower()\n",
    "    sentence = sentence.str.replace('[^A-Za-z\\s]+', ' ')\n",
    "    sentence = sentence.str.normalize('NFD')\n",
    "    sentence = sentence.str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    return sentence\n",
    "\n",
    "def read_sentence(df, lang1, lang2):\n",
    "    sentence1 = normalizeString(df, lang1)\n",
    "    sentence2 = normalizeString(df, lang2)\n",
    "    return sentence1, sentence2\n",
    "\n",
    "def read_file(loc, lang1, lang2):\n",
    "    df = pd.read_csv(loc, delimiter='\\t', header=None, names=[lang1, lang2])\n",
    "    return df\n",
    "\n",
    "def process_data(lang1,lang2):\n",
    "    df = read_file('C:\\\\Users\\\\516-29\\\\chap10\\\\data\\\\%s-%s.txt' % (lang1, lang2), lang1, lang2)\n",
    "    sentence1, sentence2 = read_sentence(df, lang1, lang2)\n",
    "\n",
    "    input_lang = Lang()\n",
    "    output_lang = Lang()\n",
    "    pairs = []\n",
    "    for i in range(len(df)):\n",
    "        if len(sentence1[i].split(' ')) < MAX_LENGTH and len(sentence2[i].split(' ')) < MAX_LENGTH:\n",
    "            full = [sentence1[i], sentence2[i]]\n",
    "            input_lang.addSentence(sentence1[i])\n",
    "            output_lang.addSentence(sentence2[i])\n",
    "            pairs.append(full)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b5047fa-b887-4a28-b7c5-1cca11c63cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(input_lang, output_lang, pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return(input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0081758-e58e-4500-a9d3-67aa08ab6a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embbed_dim, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embbed_dim = embbed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_dim, self.embbed_dim)\n",
    "        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src).view(1, 1, -1)\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f8a560d-f54b-4a3e-8fa4-b05b5ec65615",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, embbed_dim, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embbed_dim = embbed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, self.embbed_dim)\n",
    "        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
    "        self.out = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input = input.view(1, -1)\n",
    "        embedded = F.relu(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        prediction = self.softmax(self.out(output[0]))\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29deb5b7-0580-4977-9a9d-fab050cd6796",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, MAX_LENGTH=MAX_LENGTH):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input_lang, output_lang, teacher_forcing_ratio=0.5):\n",
    "        input_length = input_lang.size(0)\n",
    "        batch_size = output_lang.shape[1]\n",
    "        target_length = output_lang.shape[0]\n",
    "        vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(target_length, batch_size, vocab_size).to(self.device)\n",
    "        \n",
    "        for i in range(input_length):\n",
    "            encoder_output, encoder_hidden = self.encoder(input_lang[i])\n",
    "        decoder_hidden = encoder_hidden.to(device)\n",
    "        decoder_input = torch.tensor([SOS_token], device=device)\n",
    "\n",
    "        for t in range(target_length):\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "            outputs[t] = decoder_output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            input = (output_lang[t] if teacher_force else topi)\n",
    "            if(teacher_force == False and input.item() == EOS_token):\n",
    "                break\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18fb494b-b67c-49a5-8f14-d46254a5aa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def Model(model, input_tensor, target_tensor, model_optimizer,  criterion):\n",
    "    model_optimizer.zero_grad()\n",
    "    input_length = input_tensor.size(0)\n",
    "    loss = 0\n",
    "    epoch_loss = 0\n",
    "    output = model(input_tensor, target_tensor)\n",
    "    num_iter = output.size(0)\n",
    "\n",
    "    for ot in range(num_iter):\n",
    "        loss += criterion(output[ot], target_tensor[ot])\n",
    "    loss.backward()\n",
    "    model_optimizer.step()\n",
    "    epoch_loss = loss.item() / num_iter\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9255347-bfe5-4fc9-9359-2ec1dbe74992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, input_lang, output_lang, pairs, num_iteration=20000):\n",
    "    model.train()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss_iterations = 0\n",
    "\n",
    "    training_pairs = [tensorsFromPair(input_lang, output_lang, random.choice(pairs)) for i in range(num_iteration)]\n",
    "\n",
    "    for iter in range(1, num_iteration+1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        loss = Model(model, input_tensor, target_tensor, optimizer, criterion)\n",
    "        total_loss_iterations += loss\n",
    "\n",
    "        if iter % 5000 == 0:\n",
    "            average_loss= total_loss_iterations / 5000\n",
    "            total_loss_iterations = 0\n",
    "            print('%d %.4f' % (iter, average_loss))\n",
    "\n",
    "    torch.save(model.state_dict(), 'C:\\\\Users\\\\516-29\\\\chap10\\\\data\\\\mytraining.pt')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5879f347-5590-4454-bbaf-b7530966b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, input_lang, output_lang, sentences, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentences[0])\n",
    "        output_tensor = tensorFromSentence(output_lang, sentences[1])\n",
    "        decoded_words = []\n",
    "        output = model(input_tensor, output_tensor)\n",
    "\n",
    "        for ot in range(output.size(0)):\n",
    "            topv, topi = output[ot].topk(1)\n",
    "            \n",
    "            if topi[0].item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi[0].item()])\n",
    "\n",
    "    return decoded_words\n",
    "\n",
    "def evaluateRandomly(model, input_lang, output_lang, pairs, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('input {}'.format(pair[0]))\n",
    "        print('output {}'.format(pair[1]))\n",
    "        output_words = evaluate(model, input_lang, output_lang, pair)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('predicted {}'.format(output_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ddc347b-8b2e-4223-a09e-3129f82874ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random sentence ['how much do you charge to fix a flat tire?', 'combien prenez-vous pour reparer un pneu a plat ?']\n",
      "Input : 23191 Output : 39387\n",
      "Encoder(\n",
      "  (embedding): Embedding(23191, 256)\n",
      "  (gru): GRU(256, 512)\n",
      ")\n",
      "Decoder(\n",
      "  (embedding): Embedding(39387, 256)\n",
      "  (gru): GRU(256, 512)\n",
      "  (out): Linear(in_features=512, out_features=39387, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n",
      "5000 4.9938\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(encoder)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(decoder)\n\u001b[1;32m---> 24\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrainModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_lang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[32], line 13\u001b[0m, in \u001b[0;36mtrainModel\u001b[1;34m(model, input_lang, output_lang, pairs, num_iteration)\u001b[0m\n\u001b[0;32m     11\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m training_pair[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     12\u001b[0m target_tensor \u001b[38;5;241m=\u001b[39m training_pair[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 13\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m total_loss_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[31], line 13\u001b[0m, in \u001b[0;36mModel\u001b[1;34m(model, input_tensor, target_tensor, model_optimizer, criterion)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ot \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iter):\n\u001b[0;32m     12\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m criterion(output[ot], target_tensor[ot])\n\u001b[1;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m model_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     15\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m num_iter\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lang1 = 'eng'\n",
    "lang2 = 'fra'\n",
    "input_lang, output_lang, pairs = process_data(lang1, lang2)\n",
    "\n",
    "randomize = random.choice(pairs)\n",
    "print('random sentence {}'.format(randomize))\n",
    "\n",
    "input_size = input_lang.n_words\n",
    "output_size = output_lang.n_words\n",
    "print('Input : {} Output : {}'.format(input_size, output_size))\n",
    "\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "num_iteration = 10000\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size, embed_size, num_layers)\n",
    "decoder = Decoder(output_size, hidden_size, embed_size, num_layers)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "print(encoder)\n",
    "print(decoder)\n",
    "\n",
    "model = trainModel(model, input_lang, output_lang, pairs, num_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b54b50-3fdc-468c-aae1-6c5c2cb302ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateRandomly(model, input_lang, output_lang, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16ad44b-b24a-43e8-925d-78266ec914ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af4f539-7de0-45d1-a33b-1db28602a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(input_lang, output_lang, random.choice(pairs)) for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        loss = Model(model, input_tensor, target_tensor, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % 5000 == 0:\n",
    "            print_loss_avg = print_loss_total / 5000\n",
    "            print_loss_total = 0\n",
    "            print('%d, %.4f' % (iter, print_loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d7813bb-2e42-4430-b97f-a35de30c4835",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AttnDecoderRNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m output_size \u001b[38;5;241m=\u001b[39m output_lang\u001b[38;5;241m.\u001b[39mn_words\n\u001b[0;32m      9\u001b[0m encoder1 \u001b[38;5;241m=\u001b[39m Encoder(input_size, hidden_size, embed_size, num_layers)\n\u001b[1;32m---> 10\u001b[0m attn_decoder1 \u001b[38;5;241m=\u001b[39m \u001b[43mAttnDecoderRNN\u001b[49m(hidden_size, output_size, dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(encoder1)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(attn_decoder1)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AttnDecoderRNN' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "input_size = input_lang.n_words\n",
    "output_size = output_lang.n_words\n",
    "\n",
    "encoder1 = Encoder(input_size, hidden_size, embed_size, num_layers)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_size, dropout_p=0.1).to(device)\n",
    "print(encoder1)\n",
    "print(attn_decoder1)\n",
    "\n",
    "attn_model = trainIters(encoder1, attn_decoder1, 10000, print_every=5000, plot_every=100, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54264711-b2f7-4a89-a96c-3598bf84c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20a53b66-8a7a-4756-93c1-3fce8228e40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('C:\\\\Users\\\\516-29\\\\chap10\\\\data\\\\training.txt', sep='\\t')\n",
    "valid_df = pd.read_csv('C:\\\\Users\\\\516-29\\\\chap10\\\\data\\\\validing.txt', sep='\\t')\n",
    "test_df = pd.read_csv('C:\\\\Users\\\\516-29\\\\chap10\\\\data\\\\testing.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "352a7ca2-da27-4df9-821a-6f0046a7a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx, 1]\n",
    "        label = self.df.iloc[idx, 2]\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97957e25-7c27-47ba-9605-96233bf064f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Datasets(train_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "\n",
    "valid_dataset = Datasets(valid_df)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "\n",
    "test_dataset = Datasets(test_df)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd50e65e-3759-4505-8fa9-21ec04bf14e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\516-29\\anaconda3\\envs\\torch\\lib\\site-packages\\pytorch_transformers\\modeling_utils.py:539: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3af9acd-2043-4bb8-8223-0445b521ba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          optimizer,\n",
    "          criterion=nn.BCELoss(),\n",
    "          num_epochs=2,\n",
    "          eval_every=len(train_loader)//2,\n",
    "          best_valid_loss=float(\"Inf\")):\n",
    "\n",
    "    total_correct = 0.0\n",
    "    total_len = 0.0\n",
    "    running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    global_steps_list = []\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for text, label in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            encoded_list = [tokenizer.encode(t, add_special_tokens=True)[:128] for t in text]\n",
    "            padded_list = [e + [0] * (128-len(e)) for e in encoded_list]\n",
    "            sample = torch.tensor(padded_list)\n",
    "            sample, label = sample.to(device), label.to(device)\n",
    "            labels = torch.tensor(label)\n",
    "            outputs = model(sample, labels=labels)\n",
    "            loss, logits = outputs\n",
    "\n",
    "            pred = torch.argmax(F.softmax(logits, dim=1), dim=1)\n",
    "            correct = pred.eq(labels)\n",
    "            total_correct += correct.sum().item()\n",
    "            total_len += len(labels)\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for text, label in valid_loader:\n",
    "                        encoded_list = [tokenizer.encode(t, add_special_tokens=True)[:128] for t in text]\n",
    "                        padded_list = [e + [0] * (128-len(e)) for e in encoded_list]\n",
    "                        sample = torch.tensor(padded_list)\n",
    "                        sample, label = sample.to(device), label.to(device)\n",
    "                        labels = torch.tensor(label)\n",
    "                        outputs = model(sample, labels=labels)\n",
    "                        loss, logits = outputs\n",
    "                        valid_running_loss += loss.item()\n",
    "\n",
    "                average_train_loss = running_loss / eval_every\n",
    "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "                train_loss_list.append(average_train_loss)\n",
    "                valid_loss_list.append(average_valid_loss)\n",
    "                global_steps_list.append(global_step)\n",
    "\n",
    "                running_loss = 0.0\n",
    "                valid_running_loss = 0.0\n",
    "                model.train()\n",
    "\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader), average_train_loss, average_valid_loss))\n",
    "\n",
    "                if best_valid_loss > average_valid_loss:\n",
    "                    best_valid_loss = average_valid_loss\n",
    "                    save_checkpoint('C:\\\\Users\\\\516-29\\\\chap10\\\\data\\\\model.pt', model, best_valid_loss)\n",
    "                    save_metrics('C:\\\\Users\\\\516-29\\\\chap10\\\\data\\\\metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "\n",
    "    save_metrics('C:\\\\Users\\\\516-29\\\\chap10\\\\data\\\\metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    print('훈련 종료!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307abcef-697a-483e-871f-d39a1fd77816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\516-29\\AppData\\Local\\Temp\\ipykernel_16096\\2014735330.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(label)\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "train(model=model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b716555d-c28f-446e-be85-e29676815406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from <== C:\\Users\\516-29\\chap10\\data\\metrics.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\516-29\\AppData\\Local\\Temp\\ipykernel_9048\\3866183658.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(load_path, map_location=device)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss_list, valid_loss_list, global_steps_list = load_metrics('C:\\\\Users\\\\516-29\\\\chap10\\\\data\\\\metrics.pt')\n",
    "plt.plot(global_steps_list, train_loss_list, label='Train')\n",
    "plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
    "plt.xlabel('Global Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50ac6dde-b107-4ba2-b378-56f62774ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for text, label in test_loader:\n",
    "            encoded_list = [tokenizer.encode(t, add_special_tokens=True)[:128] for t in text]\n",
    "            padded_list = [e + [0]*(128-len(e)) if len(e)<128 else e for e in encoded_list]\n",
    "            sample = torch.LongTensor(padded_list).to(device)\n",
    "            labels = label.to(device)\n",
    "\n",
    "            loss, logits = model(sample, labels=labels)\n",
    "            y_pred.extend(torch.argmax(logits, 1).tolist())\n",
    "            y_true.extend(labels.tolist())\n",
    "\n",
    "    print('Classification 결과:')\n",
    "    print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "    ax = plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax=ax, cmap='Blues', fmt=\"d\")\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('True Labels')\n",
    "    ax.xaxis.set_ticklabels(['0', '1'])\n",
    "    ax.yaxis.set_ticklabels(['0', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2311fee8-2ad8-4639-ac84-1a6c7826e9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\516-29\\AppData\\Local\\Temp\\ipykernel_9048\\3866183658.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(load_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from <== C:\\Users\\516-29\\chap10\\data\\model.pt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m best_model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      2\u001b[0m load_checkpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m516-29\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mchap10\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m'\u001b[39m, best_model)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, test_loader)\u001b[0m\n\u001b[0;32m      8\u001b[0m encoded_list \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mencode(t, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:\u001b[38;5;241m128\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m text]\n\u001b[0;32m      9\u001b[0m padded_list \u001b[38;5;241m=\u001b[39m [e \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m128\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(e)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(e)\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m128\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m encoded_list]\n\u001b[1;32m---> 10\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_list\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m labels \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     13\u001b[0m loss, logits \u001b[38;5;241m=\u001b[39m model(sample, labels\u001b[38;5;241m=\u001b[39mlabels)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_model = model.to(device)\n",
    "load_checkpoint('C:\\\\Users\\\\516-29\\\\chap10\\\\data\\\\model.pt', best_model)\n",
    "evaluate(best_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "20501757-c0cf-4f84-95a1-324d145fc571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a0d4eb17c9435a8114862880787124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58acc628c1b4f24b84e35ad38c733b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65cacbdfeb44acbb083c0ba960143a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d45799056946d18bdd125551f3088c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "29b9656c-ba16-482a-88f0-102b274fa64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '나는', '파', '##이', '##토', '##치를', '이', '##용한', '딥', '##러', '##닝', '##을', '학', '##습', '##중', '##이다', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = \"나는 파이토치를 이용한 딥러닝을 학습중이다.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "04569dc2-f5cb-412b-8258-5493d59fdf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "과             8,898\n",
      "##수          15,891\n",
      "##원에         108,280\n",
      "사             9,405\n",
      "##과          11,882\n",
      "##가          11,287\n",
      "많             9,249\n",
      "##았다         27,303\n",
      ".               119\n",
      "친             9,781\n",
      "##구          17,196\n",
      "##가          11,287\n",
      "나             8,982\n",
      "##에게         26,212\n",
      "사             9,405\n",
      "##과          11,882\n",
      "##했다         12,490\n",
      ".               119\n",
      "백             9,331\n",
      "##설          31,928\n",
      "##공          28,000\n",
      "##주는         100,633\n",
      "독             9,088\n",
      "##이          10,739\n",
      "든             9,115\n",
      "사             9,405\n",
      "##과          11,882\n",
      "##를          11,513\n",
      "먹             9,266\n",
      "##었다         17,706\n",
      ".               119\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "text = \"과수원에 사과가 많았다.\" \\\n",
    "       \"친구가 나에게 사과했다.\" \\\n",
    "       \"백설공주는 독이 든 사과를 먹었다.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a4c54290-6bf3-43ef-9d22-61de5cca6276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "segments_ids = [1] * len(tokenized_text)\n",
    "print(segments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7df93940-3255-4e7e-b507-78b28520da47",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5b1f407d-5b55-466f-b0cf-0c58e6ddca98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9146b3c670a545aa8c2f27d55b97509d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-multilingual-cased', output_hidden_states=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3cd63234-33ba-4d58-925a-b44e83bc5239",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "    hidden_states = outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e885c1f4-83b8-47b6-a463-08bda52afba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "계층 개수: 13  (initial embeddings + 12 BERT layers)\n",
      "배치 개수: 1\n",
      "토큰 개수: 33\n",
      "은닉층의 유닛 개수: 768\n"
     ]
    }
   ],
   "source": [
    "print(\"계층 개수:\", len(hidden_states), \" (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "print(\"배치 개수:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "print(\"토큰 개수:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "print(\"은닉층의 유닛 개수:\", len(hidden_states[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2830b026-db5e-42b3-9d99-1947bc56f2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "은닉 상태의 유형: <class 'tuple'>\n",
      "각 계층에서의 텐서 형태: torch.Size([1, 33, 768])\n"
     ]
    }
   ],
   "source": [
    "print('은닉 상태의 유형:', type(hidden_states))\n",
    "print('각 계층에서의 텐서 형태:', hidden_states[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2b24b679-b108-4771-aef0-b11fc8020b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 33, 768])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1c71a619-9739-410d-ab37-9d9f2a1e1fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 33, 768])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings=torch.squeeze(token_embeddings, dim=1)\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5d765739-9006-412a-b712-2345bfa74bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 13, 768])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = token_embeddings.permute(1, 0, 2)\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e80291f2-da74-4df4-8020-86bf3efdb3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태는: 33 x 3072\n"
     ]
    }
   ],
   "source": [
    "token_vecs_cat = []\n",
    "for token in token_embeddings:\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    token_vecs_cat.append(cat_vec)\n",
    "print('형태는: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a50c2ba9-5cb3-4307-8f0a-f50f3e5269ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태는: 33 x 768\n"
     ]
    }
   ],
   "source": [
    "token_vecs_sum = []\n",
    "for token in token_embeddings:\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "print('형태는: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "942c721f-9a14-42f9-b354-656d3b946d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 임베딩 벡터의 형태: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "token_vecs = hidden_states[-2][0]\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "print(\"최종 임베딩 벡터의 형태:\", sentence_embedding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "25d7399d-6bb1-4fe0-a893-64e1a39fd59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 과\n",
      "2 ##수\n",
      "3 ##원에\n",
      "4 사\n",
      "5 ##과\n",
      "6 ##가\n",
      "7 많\n",
      "8 ##았다\n",
      "9 .\n",
      "10 친\n",
      "11 ##구\n",
      "12 ##가\n",
      "13 나\n",
      "14 ##에게\n",
      "15 사\n",
      "16 ##과\n",
      "17 ##했다\n",
      "18 .\n",
      "19 백\n",
      "20 ##설\n",
      "21 ##공\n",
      "22 ##주는\n",
      "23 독\n",
      "24 ##이\n",
      "25 든\n",
      "26 사\n",
      "27 ##과\n",
      "28 ##를\n",
      "29 먹\n",
      "30 ##었다\n",
      "31 .\n",
      "32 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(tokenized_text):\n",
    "    print(i, token_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "54f06b99-45bd-494c-8200-076c49e6f94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사과가 많았다 tensor([-0.5844, -4.0836,  0.4906,  0.8915, -1.8054])\n",
      "나에게 사과했다 tensor([-0.8631, -3.4047, -0.7351,  0.9805, -2.6700])\n",
      "사과를 먹었다 tensor([ 0.6756, -0.3618,  0.0586,  2.2050, -2.4193])\n"
     ]
    }
   ],
   "source": [
    "print(\"사과가 많았다\", str(token_vecs_sum[6][:5]))\n",
    "print(\"나에게 사과했다\", str(token_vecs_sum[10][:5]))\n",
    "print(\"사과를 먹었다\", str(token_vecs_sum[19][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c07d4b25-3859-40ec-8c52-e82e015f43dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*유사한* 의미에 대한 벡터 유사성: 0.86\n",
      "*다른* 의미에 대한 벡터 유사성: 0.91\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "diff_apple = 1 - cosine(token_vecs_sum[5], token_vecs_sum[27])\n",
    "same_apple = 1 - cosine(token_vecs_sum[5], token_vecs_sum[16])\n",
    "print('*유사한* 의미에 대한 벡터 유사성: %.2f' % same_apple)\n",
    "print('*다른* 의미에 대한 벡터 유사성: %.2f' % diff_apple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944c7b54-4ef6-4e42-89ae-0f0652ce6fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
